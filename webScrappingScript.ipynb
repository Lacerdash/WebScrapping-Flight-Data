{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from time import sleep  # Allows the program to pause for a specified amount of time\n",
    "import pandas as pd  # Provides data manipulation and analysis tools\n",
    "from selenium import webdriver  # Allows automated web browsing\n",
    "from bs4 import BeautifulSoup  # Parses HTML and XML documents\n",
    "from selenium.webdriver.common.by import By  # Provides a way to locate elements on a webpage\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Allows the program to wait for an element to load before continuing\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Allows the program to specify expected conditions for an element to load\n",
    "from datetime import date  # Provides tools for working with dates\n",
    "from unidecode import unidecode  # Allows the program to remove accents and other diacritical marks from characters\n",
    "import random  # Provides tools for generating random numbers and sequences\n",
    "import datetime  # Provides tools for working with dates and times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decolar Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_decolar_data(from_location, to_location, departure_date, arrival_date, adult_qty):\n",
    "    # Initialize search day, search ID and company name\n",
    "    search_day = date.today()\n",
    "    search_id = str(search_day) + from_location + to_location + departure_date + arrival_date\n",
    "    company = 'Decolar'\n",
    "\n",
    "    # Set up web driver options and open the search URL\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless') # Allows to run automated scripts in headless mode, meaning that the browser window wouldn't be visible.\n",
    "    with webdriver.Chrome(options=options) as driver: # Initializing webdriver\n",
    "        driver.maximize_window()\n",
    "        search_Url = 'https://www.decolar.com/shop/flights/results/roundtrip/{fromLocation}/{toLocation}/{departureDate}/{arrivalDate}/1/0/0/NA/NA/NA/NA/NA?from=SB&di={adultQty}-0&reSearch=true'\\\n",
    "        .format(fromLocation = from_location, \n",
    "                toLocation = to_location, \n",
    "                departureDate = departure_date, \n",
    "                arrivalDate = arrival_date, \n",
    "                adultQty = adult_qty\n",
    "                )\n",
    "        driver.get(search_Url)\n",
    "        \n",
    "        # Set up WebDriverWait\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Closing popup\n",
    "        popup_window = '//*[@id=\"dreck-wrongcountry-modal\"]/div[1]/i'\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, popup_window))).click() \n",
    "        except:\n",
    "            print('No popup window')\n",
    "        \n",
    "        # Closing discount popup\n",
    "        popup_discount = '//*[@id=\"header\"]/nav/div[6]/div[1]/i'\n",
    "        try:\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, popup_discount))).click() \n",
    "        except:\n",
    "            print(\"No discount popup\")\n",
    "\n",
    "        # Sorting By Cheapest\n",
    "        relXpathSortByCheapestButton = '//*[@id=\"flights-container\"]/div/div[3]/div/div[2]/div/div[4]/app-root/app-common/new-sorting-tabs/div/tab-component[2]'\n",
    "        try: \n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH, relXpathSortByCheapestButton))).click()\n",
    "        except:\n",
    "            print(\"No sort by Cheapest Button\")\n",
    "\n",
    "        # Wating until flight containers are present on the page before proceeding\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"cluster-container COMMON\"]'))) \n",
    "        \n",
    "        # Scrooling down the page to load more results, and see if the button \"load more flights has appeared\"\n",
    "        for i in range(0,5):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            sleep(1)\n",
    "\n",
    "        # Seeing if the \"load more button has appeared\" and assigning a True or False value to the variable\n",
    "        more_results_button = '//body/div/div/div/div/div/div/div/div/div/app-root/app-common/items/div/div/a[1]'\n",
    "        try:\n",
    "            status_search_see_more_flights_button = wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).is_displayed()\n",
    "        except:\n",
    "            status_search_see_more_flights_button = False\n",
    "\n",
    "        # While loop to load more fights until the button \"load more flights\" disappears\n",
    "        while status_search_see_more_flights_button == True:\n",
    "            try:\n",
    "                print(\"Loading more flights\")\n",
    "                driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\") # Scrool down te page\n",
    "                wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).click() # # Clicking the load more button when it loads\n",
    "                status_search_see_more_flights_button = wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).is_displayed() # Checking the status of the \"load more flights\"\n",
    "            except:\n",
    "                print('No more flights to load')\n",
    "                break\n",
    "\n",
    "        # Collecting Data\n",
    "        try:\n",
    "            flight_containers = driver.find_elements(By.XPATH, '//div[@class = \"cluster-container COMMON\"]')\n",
    "\n",
    "            flight_list = []\n",
    "\n",
    "            for WebElement in flight_containers:\n",
    "                elementHTML = WebElement.get_attribute('outerHTML')\n",
    "                elementSoup = BeautifulSoup(elementHTML, 'html.parser')\n",
    "\n",
    "                # Dictionary to store the flight card data\n",
    "                flight_data = {}\n",
    "\n",
    "                # SearchID, SearchDay, and serachUrl\n",
    "                flight_data['searchID'] = search_id\n",
    "                flight_data['searchDay'] = search_day\n",
    "                flight_data['searchUrl'] = search_Url\n",
    "                flight_data['departureDate'] = departure_date\n",
    "                flight_data['arrivalDate'] = arrival_date\n",
    "                flight_data['adultQty'] = adult_qty\n",
    "                flight_data['company'] = company\n",
    "                \n",
    "                # Origin Airport and Destiny Airport\n",
    "                flight_data['originAirport'] = elementSoup.find('span', class_ = 'route-location route-departure-location').text.strip().split(' ', 1)[0]\n",
    "                flight_data['destinyAirport'] = elementSoup.find('span', class_ = 'route-location route-arrival-location').text.strip().split(' ', 1)[0]\n",
    "                \n",
    "                # # Value Tarif and Taxes\n",
    "                flight_data['tarif'] = 0\n",
    "                flight_data['taxes'] = 0\n",
    "\n",
    "                # Currency and Price\n",
    "                flight_data['currency'] = elementSoup.find('span', class_ = 'currency price-mask -eva-3-mr-xsm').text\n",
    "                flight_data['value'] = elementSoup.find('span', class_ = 'amount price-amount').text.replace('.','').strip()\n",
    "\n",
    "                # CIA and CIA_abv\n",
    "                airline_img_container = elementSoup.find('span', class_='container-img-airlines')\n",
    "                list_cia_flight_container = [] # List to store the companies name, because some of them have more than 1\n",
    "                list_cia_abv_flight_container = [] # List to store the abreviated companies name, because some of them have more than 1\n",
    "                for img in airline_img_container.find_all('img'):\n",
    "                    list_cia_flight_container.append(unidecode(img['alt'].strip()))\n",
    "                    list_cia_abv_flight_container.append(unidecode(img['alt'].strip()[:4]))\n",
    "                flight_data['cia'] = list_cia_flight_container\n",
    "                flight_data['cia_abv'] = list_cia_abv_flight_container\n",
    "\n",
    "                flight_list.append(flight_data)\n",
    "\n",
    "            dataTypeDict = {\"searchID\" : 'object', 'searchDay' : 'datetime64[ns]', \"originAirport\" : 'object', \"destinyAirport\" : 'object', \n",
    "                        \"searchUrl\": 'object', \"departureDate\" : 'datetime64[ns]', \"arrivalDate\" : 'datetime64[ns]', \"adultQty\" : 'int64', \n",
    "                        \"company\" : 'object', \"cia\" : 'object', 'cia_abv' : 'object', \"currency\" : 'object', \"tarif\" : 'float32', \"taxes\" : 'float32', \n",
    "                        \"value\" : 'float32'}\n",
    "            # Creating the Data Frame\n",
    "            df = pd.DataFrame(flight_list)\n",
    "            # # Reordering the columns\n",
    "            cols = list(dataTypeDict.keys())\n",
    "            df = df[cols]\n",
    "            # # Changing the columns type\n",
    "            df = df.astype(dtype=dataTypeDict).sort_values('value', ascending=True)\n",
    "            ## Exploding the columns cia and cia_abv, because some of them have more than 1 cia separated by \",\"\n",
    "            df = df.explode(['cia', 'cia_abv'], ignore_index=True).sort_values('value', ascending=True)\n",
    "            print(f'{df.shape[0]} flights were scrapped from decolar')\n",
    "\n",
    "        except:\n",
    "            print('No flight containers found')\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Passagens Promo Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_passagens_promo_data(from_location, to_location, departure_date, arrival_date, adult_qty):\n",
    "    # Initialize search day, search ID and company name\n",
    "    search_day = date.today()\n",
    "    search_id = str(search_day) + from_location + to_location + departure_date + arrival_date\n",
    "    company = 'PP'\n",
    "\n",
    "    # Set up web driver options and open the search URL\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless') # Allows to run automated scripts in headless mode, meaning that the browser window wouldn't be visible.\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\")\n",
    "    with webdriver.Chrome(options=options) as driver:\n",
    "        search_Url = 'https://www.passagenspromo.com.br/air/search/{fromLocation}{toLocation}{departureDate}-{toLocation}{fromLocation}{arrivalDate}/{adultQty}/0/0/Y/?'\\\n",
    "        .format(\n",
    "            fromLocation = from_location, \n",
    "            toLocation = to_location, \n",
    "            departureDate = departure_date.replace('-', '')[2:], \n",
    "            arrivalDate = arrival_date.replace('-', '')[2:], \n",
    "            adultQty = adult_qty\n",
    "        )\n",
    "        driver.get(search_Url)\n",
    "\n",
    "        # Set up WebDriverWait\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # Wait for flight cards to load\n",
    "        flight_card = '//div[@class=\"flightgroupcard\"]'\n",
    "        try:\n",
    "            wait.until(EC.visibility_of_all_elements_located((By.XPATH, flight_card)))\n",
    "        except:\n",
    "            print('Not all flight cards were visible')\n",
    "        \n",
    "        # Check if \"load more\" button is visible\n",
    "        more_results_button = '//button[normalize-space()=\"Mais resultados\"]'\n",
    "        try:\n",
    "            status_search_see_more_flights_button = wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).is_displayed()\n",
    "        except:\n",
    "            print('No load more flights button was found')\n",
    "            status_search_see_more_flights_button = False\n",
    "        \n",
    "        # Load more flights while load more flights button is displayed\n",
    "        while status_search_see_more_flights_button == True:\n",
    "            try:\n",
    "                print(\"Loading more flights\")\n",
    "                driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).click()\n",
    "                status_search_see_more_flights_button = wait.until(EC.element_to_be_clickable((By.XPATH, more_results_button))).is_displayed()\n",
    "            except:\n",
    "                print('The load more flights button has dissapeared. No more flights to load')\n",
    "                break\n",
    "\n",
    "        # Collecting the Data\n",
    "        try:\n",
    "            fligths_rows = driver.find_elements(By.CLASS_NAME, \"flightcard\")\n",
    "            flight_list = []\n",
    "\n",
    "            for WebElement in fligths_rows:\n",
    "                elementHTML = WebElement.get_attribute('outerHTML')\n",
    "                elementSoup = BeautifulSoup(elementHTML, 'html.parser')\n",
    "\n",
    "                # Dictionary to store the flight card data\n",
    "                flight_data = {}\n",
    "\n",
    "                # SearchID, SearchDay, and serachUrl\n",
    "                flight_data['searchID'] = search_id\n",
    "                flight_data['searchDay'] = search_day\n",
    "                flight_data['searchUrl'] = search_Url\n",
    "                flight_data['departureDate'] = departure_date\n",
    "                flight_data['arrivalDate'] = arrival_date\n",
    "                flight_data['adultQty'] = adult_qty\n",
    "                flight_data['company'] = company\n",
    "                \n",
    "                # Origin Airport and Destiny Airport\n",
    "                flight_data['originAirport'] = elementSoup.find('label', class_ = 'origin_iata').text\n",
    "                flight_data['destinyAirport'] = elementSoup.find('label', class_ = 'destiny_iata').text\n",
    "                \n",
    "                # Value Tarif and Taxes\n",
    "                flight_data['tarif'] = elementSoup.find('span', 'value_passenger').text.split(' ', 1)[1].replace('.','')\n",
    "                flight_data['taxes'] = elementSoup.find('span', 'value_tax').text.split(' ', 1)[1].replace('.','')\n",
    "\n",
    "                # Currency and Price\n",
    "                flight_data['currency'] = elementSoup.find('div', class_ = 'total_price').text.split(' ', 1)[0]\n",
    "                flight_data['value'] = elementSoup.find('div', class_ = 'total_price').text.split(' ', 1)[1].replace('.','')\n",
    "\n",
    "                # CIA and CIA_abv\n",
    "                flight_data['cia'] = elementSoup.find('div', class_='logo_cia').text.strip()\n",
    "                flight_data['cia_abv'] = elementSoup.find('div', class_='logo_cia').text.strip()[:4]\n",
    "\n",
    "                flight_list.append(flight_data)\n",
    "\n",
    "            dataTypeDict = {\"searchID\" : 'object', 'searchDay' : 'datetime64[ns]', \"originAirport\" : 'object', \"destinyAirport\" : 'object', \n",
    "                        \"searchUrl\": 'object', \"departureDate\" : 'datetime64[ns]', \"arrivalDate\" : 'datetime64[ns]', \"adultQty\" : 'int64', \n",
    "                        \"company\" : 'object', \"cia\" : 'object', 'cia_abv' : 'object', \"currency\" : 'object', \"tarif\" : 'float32', \"taxes\" : 'float32', \n",
    "                        \"value\" : 'float32'}\n",
    "            # Creating the Data Frame\n",
    "            df = pd.DataFrame(flight_list)\n",
    "            # # Reordering the columns\n",
    "            cols = list(dataTypeDict.keys())\n",
    "            df = df[cols]\n",
    "            # # Changing the columns type\n",
    "            df = df.astype(dtype=dataTypeDict).sort_values('value', ascending=True)\n",
    "            print(f'{df.shape[0]} flights were scrapped from Passagens Promo')\n",
    "\n",
    "        except:\n",
    "            print('No flight cards found')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Merging both Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_scraper(from_location, to_location, departure_date, arrival_date, adult_qty):\n",
    "    decolar_df = scrape_decolar_data(from_location, to_location, departure_date, arrival_date, adult_qty)\n",
    "    passagens_promo_df = scrape_passagens_promo_data(from_location, to_location, departure_date, arrival_date, adult_qty)\n",
    "    return decolar_df, passagens_promo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Creating a function to transform the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(passagens_promo_df, decolar_df):\n",
    "\n",
    "    # Transforming the passagens_promo_df\n",
    "    bestPricesPerCiaPP = passagens_promo_df.groupby('cia').min().sort_values('value', ascending=True).reset_index()\n",
    "    best_cia = bestPricesPerCiaPP.loc[bestPricesPerCiaPP['value'].idxmin(), 'value'] \n",
    "    bestPricesPerCiaPP['bestCia'] = bestPricesPerCiaPP['value'].apply(lambda x: 1 if x == best_cia else 0)\n",
    "    bestPricesPerCiaPP['comparisonWithPP'] = 'PP X PP'\n",
    "\n",
    "    # Transforming the decolar_df\n",
    "    bestPricesPerCiaDecolar = decolar_df.groupby('cia').min().sort_values('value', ascending=True).reset_index()\n",
    "    best_cia = bestPricesPerCiaDecolar.loc[bestPricesPerCiaDecolar['value'].idxmin(), 'value'] \n",
    "    bestPricesPerCiaDecolar['bestCia'] = bestPricesPerCiaDecolar['value'].apply(lambda x: 1 if x == best_cia else 0)\n",
    "\n",
    "    for indexD, rowD in bestPricesPerCiaDecolar.iterrows():\n",
    "        for indexP, rowP in bestPricesPerCiaPP.iterrows():\n",
    "            if rowD.cia_abv == rowP.cia_abv:\n",
    "                if rowD.value > rowP.value:\n",
    "                    bestPricesPerCiaDecolar.loc[indexD,'comparisonWithPP'] = 'Win / Cheaper'\n",
    "                else:\n",
    "                    bestPricesPerCiaDecolar.loc[indexD,'comparisonWithPP'] = 'Loss / Expensiver'\n",
    "                    \n",
    "    bestPricesPerCiaDecolar['comparisonWithPP'].fillna('Loss / CIA not returned', inplace = True) \n",
    "\n",
    "    # Concatenating both Dataframes\n",
    "    final_df = pd.concat([bestPricesPerCiaPP, bestPricesPerCiaDecolar], axis=0).reset_index().drop(columns='index')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Creating a function to Storage the data*\n",
    "\n",
    "**ATENTTION:** You must create a excel file to storage the data and substitute the name where is writting: ``YOUR EXCEL FILE NAME HERE`` in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_spreadsheet(final_df):\n",
    "    # Loading the Spreadsheet that Storage the Scrapped Flight Data\n",
    "    fligh_data_worksheet = pd.read_excel('YOUR EXCEL FILE NAME HERE.xlsx')\n",
    "\n",
    "    # Appending the Data from the final_df into the fligh_data_worksheet if the searchID is not in the fligh_data_worksheet\n",
    "    if final_df.searchID.unique() not in fligh_data_worksheet.searchID.unique():\n",
    "        updated_fligh_data_worksheet = pd.concat([fligh_data_worksheet, final_df], axis=0).reset_index().drop(columns='index')\n",
    "        updated_fligh_data_worksheet.to_excel('YOUR EXCEL FILE NAME HERE', index=False)\n",
    "        print(\"Data appended into the spredsheet\")\n",
    "    else:\n",
    "        print(\"Data not appended into the spredsheet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Executing the functions and Loading results in an Excel File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the DataFrame with the search parameters\n",
    "search_parameters_df = pd.read_excel('search_parameters.xlsx')\n",
    "search_parameters_df['Departure Dates'] = pd.to_datetime(search_parameters_df['Departure Dates']).dt.date\n",
    "search_parameters_df['Arrival Dates'] = pd.to_datetime(search_parameters_df['Arrival Dates']).dt.date\n",
    "print(f'There is {search_parameters_df.shape[0]} rows of search parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_parameters_df)):\n",
    "    \n",
    "    # Getting the parameters for the 'search_parameters_df'\n",
    "    from_location = search_parameters_df['Departure Iata'].loc[i]\n",
    "    to_location = search_parameters_df['Arrival Iata'].loc[i]\n",
    "    departure_date = str(search_parameters_df['Departure Dates'].loc[i])\n",
    "    arrival_date = str(search_parameters_df['Arrival Dates'].loc[i])\n",
    "    adult_qty = search_parameters_df['Adult Quantity'].loc[i]\n",
    "\n",
    "    # Executing function to scrape the data\n",
    "    decolar_df, passagens_promo_df = merged_scraper(from_location, to_location, departure_date, arrival_date, adult_qty)\n",
    "\n",
    "    # Executing function to transform the data\n",
    "    final_df = transform_data(passagens_promo_df, decolar_df) \n",
    "    \n",
    "    # Loading the data into the spreadsheet\n",
    "    load_data_to_spreadsheet(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
